{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Neural Network MCMC\n",
    "Part of the Bayesian neural networks via MCMC: a Python-based tutorial\n",
    "\n",
    "This section of the tutorial covers the development of an MCMC algorithm applied to a Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import binom, uniform\n",
    "from ipywidgets import interact, fixed, widgets\n",
    "from tqdm import tqdm\n",
    "# visulisation function\n",
    "from functions.visualisations import (\n",
    "    histogram_trace, plot_y_timeseries, \n",
    "    plot_ycorr_scatter, boxplot_weights,\n",
    "    plot_linear_data\n",
    ")\n",
    "\n",
    "from types import MethodType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/project')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a class with the functions and attributes required for a neural network model\n",
    "First lets define our forward and backward pass functions which form the core of our model. These will be integrated into the NeuralNetwork class and thus contain references to class attributes.\n",
    "\n",
    "- `forward_pass`: Function to calculate the output of the network from the input data ($x$)\n",
    "    - for our hidden layer - $h_{output} = g\\left(x \\cdot w_{h} + \\delta_{h} \\right)$ \n",
    "    - subsequently for the output layer - $f\\left(x\\right) = o_{output} = g\\left(h_{output} \\cdot w_{o} + \\delta_{o} \\right)$\n",
    "    - where $g(\\cdot)$ is the sigmoid activation function, $w_{h}$ and $w_{o}$ are the weights for the hidden and output layers, and $\\delta_{h}$ and $\\delta_{o}$ are the biases for the hidden and output layers, respectively.\n",
    "- `backward_pass`: Function to update the model parameters using Langevin dynamics\n",
    "    - $\\bar{\\theta_p} = \\theta_p + r \\times \\nabla E_{y_{\\mathcal{A}_{D,T}}}\\left[\\theta_p\\right]$\n",
    "    - $\\nabla E_{y_{\\mathcal{A}_{D,T}}}\\left[\\theta_p\\right] = \\left(\\frac{\\delta E}{\\delta \\theta_1},\\dots, \\frac{\\delta E}{\\delta \\theta_L} \\right)$\n",
    "    - $E_{y_{\\mathcal{A}_{D,T}}}\\left[\\theta_p\\right] = \\sum_{t \\in \\mathcal{A}_{D,T}} \\left(y_t - f\\left(x_t\\right)^{[k]}\\right)^2$\n",
    "    - where $\\theta = \\left(w_{h},w_{o},\\delta_{h},\\delta_{o}\\right)$ comprises the weights and biases of the NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN prediction\n",
    "def forward_pass(self, X):\n",
    "    '''\n",
    "    Take an input X and return the output of the network\n",
    "    Input:\n",
    "        - X: (N x num_features) array of input data\n",
    "    Output:\n",
    "        - self.l2_output: (N) array of output data f(x) which can be \n",
    "        compared to observations (Y)\n",
    "    '''\n",
    "    # Hidden layer\n",
    "    l1_z = np.dot(X, self.l1_weights) + self.l1_biases\n",
    "    self.l1_output = self.sigmoid(l1_z) # activation function g(.)\n",
    "    # Output layer\n",
    "    l2_z = np.dot(self.l1_output, self.l2_weights) + self.l2_biases\n",
    "    self.l2_output = self.sigmoid(l2_z)\n",
    "    return self.l2_output\n",
    "\n",
    "def backward_pass(self, X, Y):\n",
    "    '''\n",
    "    Compute the gradients using a backward pass and undertake Langevin-gradient \n",
    "    updating of parameters\n",
    "    Input:\n",
    "        - X: (N x num_features) array of input data\n",
    "        - Y: (N) array of target data\n",
    "    '''\n",
    "    # dE/dtheta\n",
    "    l2_delta = (Y - self.l2_output) * (self.l2_output * (1 - self.l2_output))\n",
    "    l2_weights_delta = np.outer(\n",
    "        self.l1_output,\n",
    "        l2_delta\n",
    "    )\n",
    "    # backprop of l2_delta and same as above\n",
    "    l1_delta = np.dot(l2_delta,self.l2_weights.T) * (self.l1_output * (1 - self.l1_output))        \n",
    "    l1_weights_delta = np.outer(\n",
    "        X,\n",
    "        l1_delta\n",
    "    )\n",
    "\n",
    "    # update for output layer\n",
    "    self.l2_weights += self.lrate * l2_weights_delta\n",
    "    self.l2_biases += self.lrate * l2_delta\n",
    "    # update for hidden layer\n",
    "    self.l1_weights += self.lrate * l1_weights_delta\n",
    "    self.l1_biases += self.lrate * l1_delta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we the NeuralNetwork class, which constitutes our model. For readability we have defined the forward and backward passes above, but we will add these to the NeuralNetwork class below. The class contains the following functions:\n",
    "\n",
    "- `initialise_network`: Function to initialise the weights and biases of the network in both the hidden and output layers\n",
    "- `evaluate_proposal`: Function to assign a proposed $\\theta$ to the weights and biases and forward_pass the input data\n",
    "- `langevin_gradient`: Function to calculate the Langevin gradient based parameter updates. This uses the gradient computation in the `backward_pass` detailed above\n",
    "- `sigmoid`: Sigmoid activation function\n",
    "- `encode`: Return a vector $\\theta$ of the current weights and biases of the network\n",
    "- `decode`: Take a vector $\\theta$ and assign it to the weights and biases of the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    '''\n",
    "    Neural Network model with a single hidden layer and a single output (y)\n",
    "    '''\n",
    "    def __init__(self, layer_sizes,learning_rate=0.01):\n",
    "        '''\n",
    "        Initialize the model\n",
    "        Input:\n",
    "            - layer_sizes (input, hidden, output): array specifying the number of \n",
    "            nodes in each layer\n",
    "            - learning_rate: learning rate for the gradient update\n",
    "        '''\n",
    "        # Initial values of model parameters\n",
    "        self.input_num = layer_sizes[0]\n",
    "        self.hidden_num = layer_sizes[1]\n",
    "        self.output_num = layer_sizes[2]\n",
    "\n",
    "        # total number of parameters from weights and biases\n",
    "        self.n_params = (self.input_num * self.hidden_num) + (self.hidden_num * self.output_num) +\\\n",
    "            self.hidden_num + self.output_num\n",
    "        # learning params\n",
    "        self.lrate = learning_rate\n",
    "\n",
    "        # Initialize network structure\n",
    "        self.initialise_network()\n",
    "\n",
    "        # functions defined above - this is poor practice, but done for readability \n",
    "        # and clarity\n",
    "        self.forward_pass = MethodType(forward_pass, self)\n",
    "        self.backward_pass = MethodType(backward_pass, self)\n",
    "    \n",
    "    def initialise_network(self):\n",
    "        '''\n",
    "        Initialize network structure - weights and biases for the hidden layer\n",
    "        and output layer\n",
    "        '''\n",
    "        # hidden layer\n",
    "        self.l1_weights = np.random.normal(\n",
    "            loc=0, scale=1/np.sqrt(self.input_num),\n",
    "            size=(self.input_num, self.hidden_num))\n",
    "        self.l1_biases = np.random.normal(\n",
    "            loc=0, scale=1/np.sqrt(self.hidden_num), \n",
    "            size=(self.hidden_num,))\n",
    "        # placeholder for storing the hidden layer values\n",
    "        self.l1_output = np.zeros((1, self.hidden_num))\n",
    "\n",
    "        # output layer\n",
    "        self.l2_weights = np.random.normal(\n",
    "            loc=0, scale=1/np.sqrt(self.hidden_num), \n",
    "            size=(self.hidden_num, self.output_num))\n",
    "        self.l2_biases = np.random.normal(\n",
    "            loc=0, scale=1/np.sqrt(self.hidden_num), \n",
    "            size=(self.output_num,))\n",
    "        # placeholder for storing the model outputs\n",
    "        self.l2_output = np.zeros((1, self.output_num))\n",
    "\n",
    "    def evaluate_proposal(self, x_data, theta):\n",
    "        '''\n",
    "        A helper function to take the input data and proposed parameter sample \n",
    "        and return the prediction\n",
    "        Input:\n",
    "            data: (N x num_features) array of data\n",
    "            theta: (w,v,b_h,b_o) vector of parameters with weights and biases\n",
    "        '''\n",
    "        self.decode(theta)  # method to decode w into W1, W2, B1, B2.\n",
    "        size = x_data.shape[0]\n",
    "\n",
    "        fx = np.zeros(size)\n",
    "\n",
    "        for i in range(0, size):  # to see what fx is produced by your current weight update\n",
    "            fx[i] = self.forward_pass(x_data[i,])\n",
    "\n",
    "        return fx\n",
    "\n",
    "    def langevin_gradient(self, x_data, y_data, theta, depth):\n",
    "        '''\n",
    "        Compute the Langevin gradient based proposal distribution\n",
    "        Input:\n",
    "            - x_data: (N x num_features) array of input data\n",
    "            - y_data: (N) array of target data\n",
    "            - theta: (w,v,b_h,b_o) vector of proposed parameters.\n",
    "            - depth: SGD depth\n",
    "        Output: \n",
    "            - theta_updated: Updated parameter proposal\n",
    "\n",
    "        '''\n",
    "        self.decode(theta)  # method to decode w into W1, W2, B1, B2.\n",
    "        size = x_data.shape[0] \n",
    "        # Update the parameters based on LG \n",
    "        for _ in range(0, depth):\n",
    "            for ii in range(0, size):\n",
    "                self.forward_pass(x_data[ii,])\n",
    "                self.backward_pass(x_data[ii,], y_data[ii])\n",
    "        theta_updated = self.encode()\n",
    "        return  theta_updated\n",
    "\n",
    "    # Helper functions\n",
    "    def sigmoid(self, x):\n",
    "        '''\n",
    "        Implentation of the sigmoid function\n",
    "        '''\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "        \n",
    "    def encode(self):\n",
    "        '''\n",
    "        Encode the model parameters into a vector\n",
    "        Output:\n",
    "            - theta: vector of parameters.\n",
    "        '''\n",
    "        w1 = self.l1_weights.ravel()\n",
    "        w2 = self.l2_weights.ravel()\n",
    "        theta = np.concatenate([w1, w2, self.l1_biases, self.l2_biases])\n",
    "        return theta\n",
    "        \n",
    "    def decode(self, theta):\n",
    "        '''\n",
    "        Decode the model parameters from a vector\n",
    "        Input:\n",
    "            - theta: vector of parameters.\n",
    "        '''\n",
    "        w_layer1size = self.input_num * self.hidden_num\n",
    "        w_layer2size = self.hidden_num * self.output_num\n",
    "\n",
    "        w_layer1 = theta[0:w_layer1size]\n",
    "        self.l1_weights = np.reshape(w_layer1, (self.input_num, self.hidden_num))\n",
    "\n",
    "        w_layer2 = theta[w_layer1size:w_layer1size + w_layer2size]\n",
    "        self.l2_weights = np.reshape(w_layer2, (self.hidden_num, self.output_num))\n",
    "        self.l1_biases = theta[w_layer1size + w_layer2size:w_layer1size + w_layer2size + self.hidden_num]\n",
    "        self.l2_biases = theta[w_layer1size + w_layer2size + self.hidden_num:w_layer1size + w_layer2size + self.hidden_num + self.output_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a class for the MCMC sampling\n",
    "First we will define our `likelihood_function` and `prior_likelihood` functions. These will be integrated into the MCMC class and thus contain references to class attributes.\n",
    "\n",
    "\n",
    "- `prior_likelihood`: Function to calculate the prior likelihood\n",
    "    - We assume a Gaussian prior distribution - see Equation 22\n",
    "    - $p(\\boldsymbol{\\theta}) \\propto \\frac{1}{(2\\pi\\sigma^2)^{L/2}}\\times \n",
    " \\exp\\Bigg\\{-\\frac{1}{2\\sigma^2}\\bigg( \\sum_{i=1}^M \\theta^2_i \\bigg) \\Bigg\\}\n",
    " \\times \\tau^{-2(1+\\nu_1)}\\exp\\left(\\frac{-\\nu_2}{\\tau^2}\\right)$\n",
    "    - we implement this (using log laws) as:\n",
    "    - $\\log\\left(p\\left(\\theta\\right)\\right) = -\\frac{L}{2}\\log\\left(\\sigma^2\\right) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^M \\theta^2_i - \\left(1 + \\nu_1\\right)\\log\\left(\\tau^2\\right) - \\frac{\\nu_2}{\\tau^2}$\n",
    "    - XX we lose the pi here but not in likelihood_function - consistency\n",
    "    \n",
    "- `likelihood_function`: Function to calculate the likelihood of the data given the current weights and biases of the network\n",
    "    - Our likelihood following Equation 15 is:\n",
    "    - $p({\\mathbf Y}|X,\\boldsymbol{\\theta}) = \\frac{1}{(2\\pi \\tau ^2)^{S/2}}\\times \\exp\\left( -\\frac{1}{2\\tau^2}\\sum_{t=1}^S \\left( y_{t}- f(\\mathbf x_t, \\theta)\\right)^2\\right)$\n",
    "    - $\\log{\\left(p({\\mathbf Y}|X,\\boldsymbol{\\theta})\\right)} = -\\frac{1}{2}\\log{\\left(2\\pi\\tau^2\\right)} - \\frac{1}{2\\tau^2}\\sum_{t=1}^S \\left(y_{t}- f(\\mathbf x_t, \\theta)\\right)^2$\n",
    "    - XX we lose S here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the likelihood function\n",
    "def likelihood_function(self, theta, tausq):\n",
    "    '''\n",
    "    Calculate the likelihood of the data given the parameters\n",
    "    Input:\n",
    "        theta: (M + 1) vector of parameters. The last element of theta consitutes the bias term (giving M + 1 elements)\n",
    "        tausq: variance of the error term\n",
    "    Output:\n",
    "        log_likelihood: log likelihood of the data given the parameters\n",
    "        model_prediction: prediction of the model given the parameters\n",
    "        accuracy: accuracy (RMSE) of the model given the parameters\n",
    "    '''\n",
    "    # first make a prediction with parameters theta\n",
    "    model_prediction = self.model.evaluate_proposal(self.x_data, theta)\n",
    "    model_simulation = model_prediction + np.random.normal(0,tausq,size=model_prediction.shape) \n",
    "    accuracy = self.rmse(model_prediction, self.y_data) #RMSE error metric \n",
    "    # now calculate the log likelihood\n",
    "    log_likelihood = np.sum(-0.5 * np.log(2 * np.pi * tausq) - 0.5 * np.square(self.y_data - model_prediction) / tausq)\n",
    "    return [log_likelihood, model_prediction, model_simulation, accuracy] \n",
    "\n",
    "# Define the prior\n",
    "def prior_likelihood(self, sigma_squared, nu_1, nu_2, theta, tausq): \n",
    "    '''\n",
    "    Calculate the prior likelihood of the parameters\n",
    "    Input:\n",
    "        sigma_squared: variance of normal prior for theta\n",
    "        nu_1: parameter nu_1 of the inverse gamma prior for tau^2\n",
    "        nu_2: parameter nu_2 of the inverse gamma prior for tau^2\n",
    "        theta: (M + 1) vector of parameters. The last element of theta consitutes the bias term (giving M + 1 elements)\n",
    "        tausq: variance of the error term\n",
    "    Output:\n",
    "        log_prior: log prior likelihood\n",
    "    '''\n",
    "    n_params = self.theta_size # number of parameters in model\n",
    "    part1 = -1 * (n_params / 2) * np.log(sigma_squared)\n",
    "    part2 = 1 / (2 * sigma_squared) * (sum(np.square(theta)))\n",
    "    log_prior = part1 - part2 - (1 + nu_1) * np.log(tausq) - (nu_2 / tausq)\n",
    "    return log_prior"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the MCMC sampler as outlined in the text:\n",
    "\n",
    "<img src=\"img/LG_MCMC_algorithm.png\" width=\"50%\">\n",
    "\n",
    "- The acceptance probability is\n",
    "    - $\\alpha = \\Bigg(1, \\frac{p\\left(\\theta^p_s|Y\\right)q\\left(\\theta^{[i]}|\\theta^{p}\\right)}{p\\left(\\theta^{[i]}|Y\\right)q\\left(\\theta^{p}|\\theta^{[i]}\\right)}\\Bigg)$\n",
    "- `diff_prop` gives the q ratio\n",
    "    - $\\frac{q\\left(\\theta^{[i]}|\\theta^{p}\\right)}{q\\left(\\theta^{p}|\\theta^{[i]}\\right)}$ XX from Equation 24\n",
    "- the log posterior ratio:\n",
    "    - $\\frac{p\\left(\\theta^p_s|Y\\right)}{p\\left(\\theta^{[i]}|Y\\right)}$\n",
    "    - where the log posterior is given by the sum of the prior and likelihood:\n",
    "    - $\\log\\left(p\\left(\\theta|Y\\right)\\right) = \\log\\left(p\\left(\\theta\\right)\\right) + \\log\\left(p\\left(Y|X,\\theta\\right)\\right)$\n",
    "    - is calculated from the following parts:\n",
    "    - `diff_likelihood`: gives the likelihood - $\\log\\left(p\\left(Y|X,\\theta\\right)\\right)$ - component of log posterior ratio  \n",
    "    - `diff_priorlikelihood` gives the prior - $\\log\\left(p\\left(\\theta\\right)\\right)$ - component of log posterior ratio  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCMC sampler\n",
    "def sampler(self):\n",
    "    '''\n",
    "    Run the sampler for a defined Neural Network model\n",
    "    '''\n",
    "    # define empty arrays to store the sampled posterior values\n",
    "    # posterior of all weights and bias over all samples\n",
    "    pos_theta = np.ones((self.n_samples, self.theta_size)) \n",
    "    # posterior defining the variance of the noise in predictions\n",
    "    pos_tau = np.ones((self.n_samples, 1))\n",
    "\n",
    "    # record output f(x) over all samples\n",
    "    pred_y = np.ones((self.n_samples, self.x_data.shape[0]))\n",
    "    # record simulated values f(x) + error over all samples \n",
    "    sim_y = np.ones((self.n_samples, self.x_data.shape[0]))\n",
    "    # record the RMSE of each sample\n",
    "    rmse_data = np.zeros(self.n_samples)\n",
    "\n",
    "    ## Initialisation\n",
    "    # initialise theta\n",
    "    theta = np.random.randn(self.theta_size)\n",
    "    # make initial prediction\n",
    "    pred_y[0,] = self.model.evaluate_proposal(self.x_data, theta)\n",
    "\n",
    "    # initialise eta\n",
    "    eta = np.log(np.var(pred_y[0,] - self.y_data))\n",
    "    tau_proposal = np.exp(eta)\n",
    "\n",
    "    # Hyperpriors - considered by looking at distribution of  similar trained  models - i.e distribution of weights and bias\n",
    "    sigma_squared = self.sigma_squared\n",
    "    nu_1 = self.nu_1\n",
    "    nu_2 = self.nu_2\n",
    "\n",
    "    # calculate the prior likelihood\n",
    "    prior_likelihood = self.prior_likelihood(sigma_squared, nu_1, nu_2, theta, tau_proposal)\n",
    "    # calculate the likelihood considering observations\n",
    "    [likelihood, pred_y[0,], sim_y[0,], rmse_data[0]] = self.likelihood_function(theta, tau_proposal)\n",
    "\n",
    "    n_accept = 0  \n",
    "    n_langevin = 0\n",
    "    # Run the MCMC sample for n_samples\n",
    "    for ii in tqdm(np.arange(1,self.n_samples)):\n",
    "        # Sample new values for theta and tau\n",
    "        theta_proposal = theta + np.random.normal(0, self.step_theta, self.theta_size)\n",
    "\n",
    "        lx = np.random.uniform(0,1,1)\n",
    "        if (self.use_langevin_gradients is True) and (lx < self.l_prob):  \n",
    "            theta_gd = self.model.langevin_gradient(self.x_data, self.y_data, theta.copy(), self.sgd_depth)  \n",
    "            theta_proposal = np.random.normal(theta_gd, self.step_theta, self.theta_size)\n",
    "            theta_proposal_gd = self.model.langevin_gradient(self.x_data, self.y_data, theta_proposal.copy(), self.sgd_depth) \n",
    "\n",
    "            # for numerical reasons, we will provide a simplified implementation that simplifies\n",
    "            # the MVN of the proposal distribution\n",
    "            wc_delta = (theta - theta_proposal_gd) \n",
    "            wp_delta = (theta_proposal - theta_gd)\n",
    "\n",
    "            sigma_sq = self.step_theta\n",
    "\n",
    "            first = -0.5 * np.sum(wc_delta * wc_delta) / sigma_sq  # this is wc_delta.T  *  wc_delta /sigma_sq\n",
    "            second = -0.5 * np.sum(wp_delta * wp_delta) / sigma_sq\n",
    "\n",
    "            diff_prop =  first - second\n",
    "            n_langevin += 1\n",
    "        else:\n",
    "            diff_prop = 0\n",
    "            theta_proposal = np.random.normal(theta, self.step_theta, self.theta_size)\n",
    "\n",
    "        # eta proposal\n",
    "        eta_proposal = eta + np.random.normal(0, self.step_eta, 1)\n",
    "        tau_proposal = np.exp(eta_proposal)   \n",
    "\n",
    "        # calculate the prior likelihood\n",
    "        prior_proposal = self.prior_likelihood(\n",
    "            sigma_squared, nu_1, nu_2, theta_proposal, tau_proposal\n",
    "        )  # takes care of the gradients\n",
    "        # calculate the likelihood considering observations\n",
    "        [likelihood_proposal, pred_y[ii,], sim_y[ii,], rmse_data[ii]] = self.likelihood_function(\n",
    "            theta_proposal, tau_proposal\n",
    "        )\n",
    "\n",
    "        # since we using log scale: based on https://www.rapidtables.com/math/algebra/Logarithm.html\n",
    "        diff_likelihood = likelihood_proposal - likelihood\n",
    "        diff_priorlikelihood = prior_proposal - prior_likelihood\n",
    "        \n",
    "        mh_prob = min(1, np.exp(diff_likelihood + diff_priorlikelihood + diff_prop))\n",
    "\n",
    "        u = np.random.uniform(0, 1)\n",
    "\n",
    "        # Accept/reject\n",
    "        if u < mh_prob:\n",
    "            # Update position\n",
    "            n_accept += 1\n",
    "            # update\n",
    "            likelihood = likelihood_proposal\n",
    "            prior_likelihood = prior_proposal\n",
    "            theta = theta_proposal\n",
    "            eta = eta_proposal\n",
    "            # and store\n",
    "            pos_theta[ii,] = theta_proposal\n",
    "            pos_tau[ii,] = tau_proposal\n",
    "        else:\n",
    "            # store\n",
    "            pos_theta[ii,] = pos_theta[ii-1,]\n",
    "            pos_tau[ii,] = pos_tau[ii-1,]\n",
    "\n",
    "    # print the % of times the proposal was accepted\n",
    "    accept_ratio = (n_accept / self.n_samples) * 100\n",
    "    print(accept_ratio, '% was accepted')\n",
    "\n",
    "    # store the posterior of theta and tau, as well as the RMSE of these samples\n",
    "    self.pos_theta = pos_theta[self.n_burnin:, ]\n",
    "    self.pos_tau = pos_tau[self.n_burnin:, ] \n",
    "    self.rmse_data = rmse_data[self.n_burnin:]\n",
    "\n",
    "    # Create a pandas dataframe to store the posterior samples of theta and tau, the \n",
    "    # associated RMSE\n",
    "    results_dict = {'w{}'.format(_): self.pos_theta[:, _].squeeze() for _ in range(self.theta_size-2)}\n",
    "    results_dict['b0'] = self.pos_theta[:, self.theta_size-2].squeeze()\n",
    "    results_dict['b1'] = self.pos_theta[:, self.theta_size-1].squeeze()    \n",
    "    results_dict['tau'] = self.pos_tau.squeeze()\n",
    "    results_dict['rmse'] = self.rmse_data.squeeze()\n",
    "    \n",
    "    results_df = pd.DataFrame.from_dict(\n",
    "        results_dict\n",
    "    )\n",
    "    return results_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we the put together MCMC class, which runs the sampling of our model. For readability we have defined the some key funtions above and we will add these to the MCMC class below. The class contains the following functions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCMC:\n",
    "    def __init__(self, model, n_samples, n_burnin, x_data, y_data):\n",
    "        self.n_samples = n_samples # number of MCMC samples\n",
    "        self.n_burnin = n_burnin # number of burn-in samples\n",
    "        self.x_data = x_data # (N x num_features)\n",
    "        self.y_data = y_data # (N x 1)\n",
    "\n",
    "        # MCMC parameters - defines how much variation you need in changes to theta, tau\n",
    "        self.step_theta = 0.025;  \n",
    "        self.step_eta = 0.2; # note eta is used as tau in the sampler to consider log scale.\n",
    "        # Hyperpriors\n",
    "        self.sigma_squared = 25\n",
    "        self.nu_1 = 0\n",
    "        self.nu_2 = 0\n",
    "\n",
    "\n",
    "        # initisalise the linear model class\n",
    "        self.model = model\n",
    "        self.use_langevin_gradients = True\n",
    "        self.sgd_depth = 1\n",
    "        self.l_prob = 0.5 # likelihood prob\n",
    "        self.theta_size = self.model.n_params # weights for each feature and a bias term\n",
    "\n",
    "        # store output\n",
    "        self.pos_theta = None\n",
    "        self.pos_tau = None\n",
    "        self.rmse_data = None\n",
    "\n",
    "        # functions defined above - this is poor practice, but done for readability \n",
    "        # and clarity\n",
    "        self.likelihood_function = MethodType(likelihood_function, self)\n",
    "        self.prior_likelihood = MethodType(prior_likelihood, self)\n",
    "        self.sampler = MethodType(sampler, self)\n",
    "\n",
    "    def rmse(self, predictions, targets):\n",
    "        '''\n",
    "        Additional error metric - root mean square error\n",
    "        '''\n",
    "        return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "    def model_draws(self, num_draws = 10, verbose=False):\n",
    "        '''\n",
    "        Calculate the output of the network from draws of the posterior distribution\n",
    "        Input:\n",
    "            - num_draws: number of draws\n",
    "            - verbose: if True, print the details of each draw\n",
    "        Output:\n",
    "            - pred_y: (num_draws x N) ouptut of the NN for each draw\n",
    "        '''\n",
    "        accuracy = np.zeros(num_draws)\n",
    "        rmse_data = np.zeros(num_draws) \n",
    "        pred_y = np.zeros((num_draws, self.x_data.shape[0]))\n",
    "        sim_y = np.zeros((num_draws, self.x_data.shape[0]))\n",
    "\n",
    "        for ii in range(num_draws):\n",
    "            theta_drawn = np.random.normal(self.pos_theta.mean(axis=0), self.pos_theta.std(axis=0), self.theta_size)\n",
    "            tausq_drawn = np.random.normal(self.pos_tau.mean(), self.pos_tau.std())\n",
    "\n",
    "            [likelihood_proposal, pred_y[ii,], sim_y[ii,], rmse_data[ii]] = self.likelihood_function(\n",
    "                theta_drawn, tausq_drawn\n",
    "            )\n",
    "            if verbose:\n",
    "                print(\n",
    "                    'Draw {} - RMSE: {:.3f}. Theta: {}, Tau {}'.format(\n",
    "                        ii, rmse_data[ii], theta_drawn, tausq_drawn\n",
    "                    )\n",
    "                )\n",
    "        return pred_y, sim_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "- Load in the suspot data\n",
    "- You can also load in the other regeression datasets `Lazer` and `Energy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "train_data   = np.loadtxt(\"data/Sunspot/train.txt\")\n",
    "test_data    = np.loadtxt(\"data/Sunspot/test.txt\")\n",
    "name        = \"Sunspot\"\n",
    "hidden      = 5\n",
    "input       = 4  #\n",
    "output      = 1\n",
    "prob_type   = 'regression' \n",
    "\n",
    "print('Training data shape: {}'.format(train_data.shape))\n",
    "print('\\n'.join(['Variable']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample using MCMC\n",
    "\n",
    "- Create the MCMC loop and sample the posterior distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MCMC Settings and Setup\n",
    "n_samples       = 25000 # number of samples to draw from the posterior\n",
    "burn_in         = int(n_samples* 0.5) # number of samples to discard before recording draws from the posterior\n",
    "learning_rate   = 0.01\n",
    "\n",
    "# Generate toy data\n",
    "n_data      = 100\n",
    "n_features  = 2\n",
    "x_data      = np.repeat(np.expand_dims(np.linspace(0, 1, n_data),axis=-1),n_features,axis=1)\n",
    "y_data      = 0.1 * x_data[:,0] + 0.5 * (x_data[:,1] ** 2)  + 0.1 + np.random.randn(n_data) * 0.05\n",
    "y_data      = y_data \n",
    "layer_sizes = [n_features, 10, 1]\n",
    "\n",
    "# or load from sunspot data\n",
    "x_data = train_data[:,:-1]\n",
    "y_data = train_data[:,-1]\n",
    "layer_sizes = [x_data.shape[1], hidden, 1]\n",
    "\n",
    "# Initialise the MCMC class\n",
    "nn_model = NeuralNetwork(layer_sizes,learning_rate)\n",
    "mcmc = MCMC(nn_model,n_samples, burn_in, x_data, y_data)\n",
    "\n",
    "# Run the sampler\n",
    "results = mcmc.sampler()\n",
    "# Draw sample models from the posterior\n",
    "pred_y, sim_y = mcmc.model_draws(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaulate test dataset\n",
    "mcmc.x_data = test_data[:,:-1]\n",
    "y_test_data = test_data[:,-1]\n",
    "mcmc.y_data = y_test_data\n",
    "\n",
    "pred_y_test, sim_y_test = mcmc.model_draws(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse the results\n",
    "Plot the data with the mean linear fit and some uncertainty.\n",
    "\n",
    "Plot the posterior distribution and trace for each parameter using ipywidgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data with the model predictions from posterior draws\n",
    "plot_ycorr_scatter(\n",
    "    y_data,\n",
    "    pred_y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_y_timeseries(\n",
    "    y_data,\n",
    "    pred_y,\n",
    "    dataset_name=name + ' Train',\n",
    "    ci=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_y_timeseries(\n",
    "    y_test_data,\n",
    "    pred_y_test,\n",
    "    dataset_name=name + ' Test',\n",
    "    ci=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the train/test RMSE\n",
    "print('Train RMSE: {:.3f}'.format(mcmc.rmse(pred_y.mean(axis=0), y_data)))\n",
    "print('Test RMSE: {:.3f}'.format(mcmc.rmse(pred_y_test.mean(axis=0), y_test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(results, param_name):\n",
    "    # results = results_rmse\n",
    "    posterior_mean = results[param_name].mean()\n",
    "    print('{:.3f} mean value of posterior'.format(posterior_mean))\n",
    "    histogram_trace(results[param_name].values)\n",
    "\n",
    "# use ipywidgets to get a \"gui\" dropdown to view all the parameters\n",
    "interact(\n",
    "    plot_hist, \n",
    "    results=fixed(results), \n",
    "    param_name=widgets.Dropdown(\n",
    "        options=results.columns,\n",
    "        value='w0',\n",
    "        description='Parameter:',\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise all the network weights\n",
    "boxplot_weights(results,width=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "87fbea7b3721842a93ed4da8a1ac4b18f42b1eaaedefc3a2702202c09bf233e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
