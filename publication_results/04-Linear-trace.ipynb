{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model MCMC\n",
    "Part of the Bayesian neural networks via MCMC: a Python-based tutorial\n",
    "\n",
    "This section of the tutorial covers the development of an MCMC algorithm applied to a simple linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ipywidgets import interact, fixed, widgets\n",
    "# visulisation function\n",
    "sys.path.append('/project')\n",
    "from functions.visualisations import (\n",
    "    histogram_trace, plot_linear_data,\n",
    "    plot_y_timeseries, boxplot_weights,\n",
    "    plot_ycorr_scatter\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from types import MethodType\n",
    "\n",
    "np.random.seed(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/project')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a class with the functions and attributes required for a linear model\n",
    "\n",
    "- `predict`: Function to output y given the input data and model parameters - $y = w x + b$\n",
    "- `evaluate_proposal`: Function to load a given proposal distribution ($\\theta$) and return the model prediction\n",
    "- `encode`: Helper function to encode the model parameters ($\\theta$) into the model as $w$ and $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.linear_model import LinearModel\n",
    "from models.mcmc import MCMC_Linear as MCMC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "- Load in the suspot data\n",
    "- You can also load in the other regeression datasets `Lazer` and `Energy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (105, 5)\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "name        = \"Abalone\"\n",
    "train_data   = np.loadtxt(\"data/{}/train.txt\".format(name))\n",
    "test_data    = np.loadtxt(\"data/{}/test.txt\".format(name))\n",
    "\n",
    "print('Training data shape: {}'.format(train_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample using MCMC\n",
    "\n",
    "- Create the MCMC loop and sample the posterior distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 143501/499999 [03:28<08:37, 688.44it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m mcmc \u001b[39m=\u001b[39m MCMC(lm,n_samples, burn_in, x_data, y_data, x_test, y_test)\n\u001b[1;32m     33\u001b[0m \u001b[39m# Run the sampler\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m results, pred \u001b[39m=\u001b[39m mcmc\u001b[39m.\u001b[39;49msampler()\n\u001b[1;32m     35\u001b[0m \u001b[39m# thin to remove autocorrelation and to reduce the size of the data \u001b[39;00m\n\u001b[1;32m     36\u001b[0m results \u001b[39m=\u001b[39m results\u001b[39m.\u001b[39miloc[::thin_factor,:]\n",
      "File \u001b[0;32m/project/publication_results/models/mcmc.py:274\u001b[0m, in \u001b[0;36mMCMC_Linear.sampler\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    271\u001b[0m tausq_proposal \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(eta_proposal)   \n\u001b[1;32m    273\u001b[0m \u001b[39m# calculate the prior likelihood\u001b[39;00m\n\u001b[0;32m--> 274\u001b[0m prior_proposal \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprior_likelihood(\n\u001b[1;32m    275\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msigma_squared, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnu_1, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnu_2, theta_proposal, tausq_proposal\n\u001b[1;32m    276\u001b[0m )\n\u001b[1;32m    277\u001b[0m \u001b[39m# calculate the likelihood considering observations\u001b[39;00m\n\u001b[1;32m    278\u001b[0m [likelihood_proposal, pred_y[ii,], sim_y[ii,], rmse_data[ii]] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlikelihood_function(\n\u001b[1;32m    279\u001b[0m     theta_proposal, tausq_proposal\n\u001b[1;32m    280\u001b[0m )\n",
      "File \u001b[0;32m/project/publication_results/models/mcmc.py:166\u001b[0m, in \u001b[0;36mMCMC.classification_prior_likelihood\u001b[0;34m(self, sigma_squared, nu_1, nu_2, theta, tausq)\u001b[0m\n\u001b[1;32m    164\u001b[0m n_params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtheta_size \u001b[39m# number of parameters in model\u001b[39;00m\n\u001b[1;32m    165\u001b[0m part1 \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39m*\u001b[39m (n_params \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mlog(sigma_squared)\n\u001b[0;32m--> 166\u001b[0m part2 \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m/\u001b[39m (\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m sigma_squared) \u001b[39m*\u001b[39m (\u001b[39msum\u001b[39;49m(np\u001b[39m.\u001b[39;49msquare(theta)))\n\u001b[1;32m    167\u001b[0m log_prior \u001b[39m=\u001b[39m part1 \u001b[39m-\u001b[39m part2\n\u001b[1;32m    168\u001b[0m \u001b[39mreturn\u001b[39;00m log_prior\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_samples = 500000 # number of samples to draw from the posterior\n",
    "burn_in = 100000 # number of samples to discard before recording draws from the posterior\n",
    "\n",
    "thin_factor = 50\n",
    "\n",
    "# or load from sunspot data\n",
    "x_data = train_data[:,:-1]\n",
    "y_data = train_data[:,-1]\n",
    "x_test = test_data[:,:-1]\n",
    "y_test = test_data[:,-1]\n",
    "\n",
    "if name in ['Sunspot','Abalone']:\n",
    "    layer_sizes = [x_data.shape[1], 1]\n",
    "    data_case = 'regression'\n",
    "elif name in ['Iris']:\n",
    "    layer_sizes = [x_data.shape[1], 3]\n",
    "    data_case = 'classification'\n",
    "elif name in ['Ionosphere']:\n",
    "    layer_sizes = [x_data.shape[1], 2]\n",
    "    data_case = 'classification'\n",
    "else:\n",
    "    raise ValueError('data_case is invalid.')\n",
    "\n",
    "thinned_res = []\n",
    "\n",
    "## MCMC Settings and Setup\n",
    "for this_chain in np.arange(5):\n",
    "    print('Chain: {}'.format(this_chain))\n",
    "    np.random.seed(2023 + this_chain)\n",
    "    # Initialise the MCMC class\n",
    "    lm = LinearModel(layer_sizes=layer_sizes,data_case=data_case)\n",
    "    mcmc = MCMC(lm,n_samples, burn_in, x_data, y_data, x_test, y_test)\n",
    "    # Run the sampler\n",
    "    results, pred = mcmc.sampler()\n",
    "    # thin to remove autocorrelation and to reduce the size of the data \n",
    "    results = results.iloc[::thin_factor,:]\n",
    "    for _ in pred.keys():\n",
    "        pred[_] = pred[_][::thin_factor,:]\n",
    "\n",
    "    thinned_res.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather the predicitons into useful variables\n",
    "pred_y = pred['train_pred']\n",
    "sim_y = pred['train_sim']\n",
    "pred_y_test = pred['test_pred']\n",
    "sim_y_test = pred['test_sim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thinned_res[0].shape\n",
    "\n",
    "conc_res = np.stack(thinned_res,axis=0)\n",
    "# results = pd.concat(thinned_res, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conc_res.shape)\n",
    "\n",
    "from convergence.convergence import gelman_rubin\n",
    "# run gelman rubin test\n",
    "gr = gelman_rubin(conc_res)\n",
    "\n",
    "# print the results into pandas then print\n",
    "gr_df = pd.DataFrame(data = gr, index=thinned_res[0].columns).T\n",
    "gr_df.index = ['Rhat']\n",
    "gr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse the results\n",
    "Plot the data with the mean linear fit and some uncertainty.\n",
    "\n",
    "Plot the posterior distribution and trace for each parameter using ipywidgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the train/test RMSE\n",
    "trainRMSE = np.array([mcmc.rmse(pred_y[_,:], y_data) for _ in np.arange(pred_y.shape[0])])\n",
    "testRMSE = np.array([mcmc.rmse(pred_y_test[_,:], y_test) for _ in np.arange(pred_y_test.shape[0])])\n",
    "\n",
    "print('Train RMSE: {:.5f} ({:.5f})'.format(trainRMSE.mean(),trainRMSE.std()))\n",
    "print('Test RMSE: {:.5f} ({:.5f})'.format(testRMSE.mean(),testRMSE.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(results, param_name):\n",
    "    # results = results_rmse\n",
    "    posterior_mean = results[param_name].mean()\n",
    "    print('{:.3f} mean value of posterior'.format(posterior_mean))\n",
    "    histogram_trace(results[param_name].values)\n",
    "\n",
    "# use ipywidgets to get a \"gui\" dropdown to view all the parameters\n",
    "interact(\n",
    "    plot_hist, \n",
    "    results=fixed(results), \n",
    "    param_name=widgets.Dropdown(\n",
    "        options=results.columns,\n",
    "        value='w0',\n",
    "        description='Parameter:',\n",
    "    )\n",
    ")\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_context('talk')\n",
    "sns.set_style('white')\n",
    "\n",
    "# Now plot posterior distribution and trace plot for selected parameters\n",
    "def plot_posterior(this_model, this_data, this_param, save_dir=None):\n",
    "    '''\n",
    "    Plot the posterior as a histogram and then the traceplot\n",
    "    '''\n",
    "    plot_data = results[this_param]\n",
    "    fig, ax = plt.subplots(1,1,figsize=(6,4))\n",
    "    # plot the posterior distribution\n",
    "    sns.histplot(plot_data.values, ax=ax, bins=20, edgecolor='xkcd:dark grey', linewidth=1.5)\n",
    "    ax.set_xlabel(this_param, labelpad=10)\n",
    "    ax.set_ylabel('Frequency', labelpad=10)\n",
    "    ax.set_title('Posterior distribution', pad=10)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # plot the trace plot\n",
    "    fig1, ax1 = plt.subplots(1,1,figsize=(6,4))\n",
    "    sns.lineplot(x=np.arange(plot_data.shape[0]).values,y=plot_data.values,ax=ax1)\n",
    "    ax1.set_xlabel('Samples', labelpad=10)\n",
    "    ax1.set_ylabel(this_param, labelpad=10)\n",
    "    ax1.set_title('Trace plot', pad=10)\n",
    "\n",
    "    fig1.tight_layout()\n",
    "    \n",
    "    if not save_dir is None:\n",
    "        fig.savefig(os.path.join(save_dir,'{}_{}_{}_posterior.png'.format(this_model,this_data,this_param)))\n",
    "        fig1.savefig(os.path.join(save_dir,'{}_{}_{}_trace.png'.format(this_model,this_data,this_param)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = os.path.join('publication_results','results')\n",
    "fig_dir = os.path.join(\n",
    "    results_dir,\n",
    "    'figures',\n",
    "    'posterior'\n",
    ")\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "\n",
    "plot_posterior('linear_tp',name,'w0')#,save_dir=fig_dir)\n",
    "# plot_posterior('bnn_tp',name,'w1')#,save_dir=fig_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "87fbea7b3721842a93ed4da8a1ac4b18f42b1eaaedefc3a2702202c09bf233e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
